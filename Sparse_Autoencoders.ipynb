{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyP3Y/P3FlxBKcsaSQ9sVsyQ",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kutluhanNG/MachineLearning/blob/main/Sparse_Autoencoders.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Sparse Autoencoders**\n",
        "\n",
        "By adding the right term to the cost function, the autoencoder is pushed to reduce the number of active neurons in the coding layer.\n",
        "\n",
        "This forces the autoencoder to represent each input as a combination of a small number of activations. As a result, each neuron in the coding layer typically ends up representing a useful feature.\n",
        "\n",
        "1) Use sigmoid activation in the coding layer > to constrain the codings to take value between 0 and 1).\n",
        "\n",
        "2) Use a large coding layer.\n",
        "\n",
        "3) *Add l1 regularization to the coding layer's activations.*\n",
        "\n",
        "4) Decoder is just a regular decoder."
      ],
      "metadata": {
        "id": "v4RyNydI27yL"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "RwmTNIyH2zKD"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "sparse_l1_encoder = tf.keras.Sequential([\n",
        "    tf.keras.layers.Flatten(),\n",
        "    tf.keras.layers.Dense(100, activation=\"relu\"),\n",
        "    tf.keras.layers.Dense(300, activation=\"sigmoid\"),\n",
        "    tf.keras.layers.ActivityRegularization(l1=1e-4),\n",
        "\n",
        "])\n",
        "\n",
        "sparse_l1_decoder = tf.keras.Sequential([\n",
        "    tf.keras.layers.Dense(100, activation=\"relu\"),\n",
        "    tf.keras.layers.Dense(28 * 28),\n",
        "    tf.keras.layers.Reshape([28, 28]),\n",
        "])\n",
        "\n",
        "sparse_l1_ae = tf.keras.Sequential([sparse_l1_encoder, sparse_l1_decoder])"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This ActivityRegularization layer just returns its inputs, but as a side effect it adds a training loss equal to the sum of the absolute values of its inputs.This only\n",
        "affects training. Equivalently, you could remove the ActivityRegularization layer\n",
        " and set activity_regularizer=tf.keras.regularizers.l1(1e-4) in the previous\n",
        " layer. This penalty will encourage the neural network to produce codings close to\n",
        " 0, but since it will also be penalized if it does not reconstruct the inputs correctly,\n",
        " it will have to output at least a few nonzero values. Using the ℓ1\n",
        " norm rather than\n",
        " the ℓ2\n",
        " norm will push the neural network to preserve the most important codings\n",
        " while eliminating the ones that are not needed for the input image (rather than just\n",
        " reducing all codings)"
      ],
      "metadata": {
        "id": "IoB3Ht7n9DmU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Sparse Autoencoder Based on Kullback-Leibler Divergence**"
      ],
      "metadata": {
        "id": "NIM0LgWIAZrJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "kl_divergence = tf.keras.losses.kullback_leibler_divergence\n",
        "\n",
        "class KLDivergenceRegularizer(tf.keras.regularizers.Regularizer):\n",
        "    def __init__(self, weight, target):\n",
        "      self.weight = weights\n",
        "      self.target = target\n",
        "\n",
        "    def __call__(self, inputs):\n",
        "      mean_activities = tf.reduce_mean(inputs, axis=0)\n",
        "      return self.weight * kl_divergence(self.target, mean_activities) + kl_divergence(1. - self.target, 1. - mean_activities)"
      ],
      "metadata": {
        "id": "U28eFOuF6wnj"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "kld_reg = KLDivergenceRegularizer(weight=5e-3, target=0.1)\n",
        "sparse_kl_encoder = tf.keras.Sequential([\n",
        "  tf.keras.layers.Flatten(),\n",
        "  tf.keras.layers.Dense(100, activation=\"relu\"),\n",
        "  tf.keras.layers.Dense(300, activation=\"sigmoid\", activity_regularizer=kld_reg)\n",
        "])\n",
        "\n",
        "sparse_kl_decoder = tf.keras.Sequential([\n",
        "  tf.keras.layers.Dense(100, activation=\"relu\"),\n",
        "  tf.keras.layers.Dense(28 * 28),\n",
        "  tf.keras.layers.Reshape([28, 28])\n",
        "])\n",
        "\n",
        "sparse_kl_ae = tf.keras.Sequential([sparse_kl_encoder, sparse_kl_decoder])"
      ],
      "metadata": {
        "id": "42zd92TnBch3"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}