{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMvdU4k3z12hQbt4UIId6z6",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kutluhanNG/MachineLearning/blob/main/Denoising_Autoencoders.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "Jy0iMNio_KRT"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "dropout_encoder = tf.keras.Sequential([\n",
        "    tf.keras.layers.Flatten(),\n",
        "    tf.keras.layers.Dropout(0.5),\n",
        "    tf.keras.layers.Dense(100, activation=\"relu\"),\n",
        "    tf.keras.layers.Dense(30, activation=\"relu\")\n",
        "])\n",
        "\n",
        "dropout_decoder = tf.keras.Sequential([\n",
        "    tf.keras.layers.Dense(100, activation=\"relu\"),\n",
        "    tf.keras.layers.Dense(28 * 28),\n",
        "    tf.keras.layers.Reshape([28, 28])\n",
        "])\n",
        "\n",
        "dropout_aq = tf.keras.Sequential([dropout_encoder, dropout_decoder])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Autoencoders are neural networks used for unsupervised learning of efficient codings, primarily for dimensionality reduction or feature learning. Introducing sparsity as a constraint means that the network is trained to have only a few neurons active at any given time in the coding layer. This is accomplished by adding a sparsity term to the loss function, which penalizes the network when too many neurons are active. For instance, enforcing that only 5% of the neurons are significantly active forces the autoencoder to represent inputs with a limited number of activations. This selective activation ensures that each active neuron learns to represent a specific, useful feature, much like using a limited vocabulary to convey meaningful information effectively.\n",
        "\n",
        "To implement sparsity, one common approach is to use the sigmoid activation function in the coding layer. The sigmoid function constrains the output values between 0 and 1, making it easier to apply regularization techniques like ℓ₁ regularization, which promotes sparsity by penalizing the absolute values of the activations. A large coding layer, such as one with 300 units, provides ample capacity for feature representation while the sparsity constraint ensures that only a small subset of these units are active for any input. The decoder part of the autoencoder remains a standard decoder, focusing on reconstructing the input from the sparse code without additional constraints.\n",
        "\n",
        "By enforcing sparsity, the autoencoder becomes more efficient in learning meaningful and distinct features from the input data. This approach not only improves the quality of feature extraction but also enhances the network's ability to generalize by preventing overfitting through the reduction of unnecessary active neurons."
      ],
      "metadata": {
        "id": "wi-DkH-HE4_8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sparse_l1_encoder = tf.keras.Sequential([\n",
        "    tf.keras.layers.Flatten(),\n",
        "    tf.keras.layers.Dense(100, activation=\"relu\"),\n",
        "    tf.keras.layers.Dense(300, activation=\"sigmoid\"),\n",
        "    tf.keras.layers.ActivityRegularization(l1=1e-4)\n",
        "])\n",
        "\n",
        "sparse_l1_decoder = tf.keras.Sequential([\n",
        "    tf.keras.layers.Dense(100, activation=\"relu\"),\n",
        "    tf.keras.layers.Dense(28 * 28),\n",
        "    tf.keras.layers.Reshape([28, 28])\n",
        "])\n",
        "\n",
        "sparse_l1_aq = tf.keras.Sequential([\n",
        "    sparse_l1_encoder,\n",
        "    sparse_l1_decoder\n",
        "])"
      ],
      "metadata": {
        "id": "1vi1R6WNBeXn"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "dD3wpBoJFd9q"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}