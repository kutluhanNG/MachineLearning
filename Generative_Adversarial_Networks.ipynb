{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "authorship_tag": "ABX9TyOGxRuyfnPDIBFGzE/Y19m5",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kutluhanNG/MachineLearning/blob/main/Generative_Adversarial_Networks.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.datasets import fashion_mnist\n",
        "\n",
        "# Load the Fashion MNIST dataset.\n",
        "# The dataset comes split into a training set and a test set.\n",
        "(X_train_full, y_train_full), (X_test, y_test) = fashion_mnist.load_data()\n",
        "\n",
        "# Optionally, normalize the pixel values to the [0, 1] range.\n",
        "X_train_full = X_train_full.astype('float32') / 255.0\n",
        "X_test = X_test.astype('float32') / 255.0\n",
        "\n",
        "# Create a validation set by splitting off a portion of the training data.\n",
        "# For example, use 10% of the training data as validation.\n",
        "validation_ratio = 0.1\n",
        "num_validation = int(validation_ratio * X_train_full.shape[0])\n",
        "\n",
        "X_valid = X_train_full[:num_validation]\n",
        "y_valid = y_train_full[:num_validation]\n",
        "\n",
        "X_train = X_train_full[num_validation:]\n",
        "y_train = y_train_full[num_validation:]\n",
        "\n",
        "# Display the shapes of the resulting datasets.\n",
        "print(\"X_train shape:\", X_train.shape)   # Expected: (54000, 28, 28) if using 10% for validation\n",
        "print(\"y_train shape:\", y_train.shape)\n",
        "print(\"X_valid shape:\", X_valid.shape)   # Expected: (6000, 28, 28)\n",
        "print(\"y_valid shape:\", y_valid.shape)\n",
        "print(\"X_test shape:\", X_test.shape)     # Expected: (10000, 28, 28)\n",
        "print(\"y_test shape:\", y_test.shape)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iHX2-HLwx-Hm",
        "outputId": "e7fbb8e9-0477-4b68-d599-e4dcb5e7de0c"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/train-labels-idx1-ubyte.gz\n",
            "\u001b[1m29515/29515\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 0us/step\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/train-images-idx3-ubyte.gz\n",
            "\u001b[1m26421880/26421880\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 0us/step\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/t10k-labels-idx1-ubyte.gz\n",
            "\u001b[1m5148/5148\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 0us/step\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/t10k-images-idx3-ubyte.gz\n",
            "\u001b[1m4422102/4422102\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 0us/step\n",
            "X_train shape: (54000, 28, 28)\n",
            "y_train shape: (54000,)\n",
            "X_valid shape: (6000, 28, 28)\n",
            "y_valid shape: (6000,)\n",
            "X_test shape: (10000, 28, 28)\n",
            "y_test shape: (10000,)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "QWts-Nv3u96z"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "codings_size = 30\n",
        "\n",
        "Dense = tf.keras.layers.Dense\n",
        "\n",
        "generator = tf.keras.Sequential([\n",
        "    Dense(100, activation=\"relu\", kernel_initializer=\"he_normal\"),\n",
        "    Dense(150, activation=\"relu\", kernel_initializer=\"he_normal\"),\n",
        "    Dense(28 * 28, activation=\"sigmoid\"),\n",
        "    tf.keras.layers.Reshape([28, 28])\n",
        "])\n",
        "\n",
        "discriminator = tf.keras.Sequential([\n",
        "    tf.keras.layers.Flatten(),\n",
        "    Dense(150, activation=\"relu\", kernel_initializer=\"he_normal\"),\n",
        "    Dense(100, activation=\"relu\", kernel_initializer=\"he_normal\"),\n",
        "    Dense(1, activation=\"sigmoid\")\n",
        "])\n",
        "\n",
        "gan = tf.keras.Sequential([generator, discriminator])\n",
        "\n",
        "discriminator.compile(loss=\"binary_crossentropy\", optimizer=\"rmsprop\")\n",
        "discriminator.trainable = False\n",
        "\n",
        "gan.compile(loss=\"binary_crossentropy\", optimizer=\"rmsprop\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size = 32\n",
        "dataset = tf.data.Dataset.from_tensor_slices(X_train).shuffle(1000)\n",
        "dataset = dataset.batch(batch_size, drop_remainder=True).prefetch(1)\n"
      ],
      "metadata": {
        "id": "64XwPGSbuhYV"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_gan(gan, dataset, batch_size, codings_size, n_epochs):\n",
        "  generator, discriminator = gan.layers\n",
        "\n",
        "  for epoch in range(n_epochs):\n",
        "    for X_batch in dataset:\n",
        "\n",
        "      # Phase 1. Only the discriminator is trainable in this phase.\n",
        "      noise = tf.random.normal(shape=[batch_size, codings_size])\n",
        "      generated_images = generator(noise)\n",
        "      X_fake_and_real = tf.concat([generated_images, X_batch], axis=0)\n",
        "      y1 = tf.constant([[0.]] * batch_size + [[1.]] * batch_size)\n",
        "      discriminator.train_on_batch(X_fake_and_real, y1)\n",
        "\n",
        "      # Phase 2. Only the generator is trainable in this phase.\n",
        "      noise = tf.random.normal(shape=[batch_size, codings_size])\n",
        "      y2 = tf.constant([[1.]] * batch_size)\n",
        "      gan.train_on_batch(noise, y2)\n",
        "\n",
        "train_gan(gan, dataset, batch_size, codings_size, n_epochs=10)"
      ],
      "metadata": {
        "id": "RnDfm-YhyyY2"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "codings = tf.random.normal(shape=[batch_size, codings_size])\n",
        "generated_images = generator.predict(codings)"
      ],
      "metadata": {
        "id": "Z895eTqy0Ie7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bf8d828b-8128-4267-d7a8-9bfb1d73360c"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 63ms/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Vpt0k467lkbT"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}